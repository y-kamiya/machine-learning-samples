TransformerModel(
  (position_embeddings): Embedding(512, 768)
  (lang_embeddings): Embedding(2, 768)
  (embeddings): Embedding(247, 768, padding_idx=2)
  (layer_norm_emb): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=768, out_features=247, bias=True)
  )
)
TransformerModel(
  (position_embeddings): Embedding(512, 768)
  (lang_embeddings): Embedding(2, 768)
  (embeddings): Embedding(247, 768, padding_idx=2)
  (layer_norm_emb): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=768, out_features=3072, bias=True)
      (lin2): Linear(in_features=3072, out_features=768, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=768, out_features=768, bias=True)
      (k_lin): Linear(in_features=768, out_features=768, bias=True)
      (v_lin): Linear(in_features=768, out_features=768, bias=True)
      (out_lin): Linear(in_features=768, out_features=768, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=768, out_features=247, bias=True)
  )
)

'position_embeddings.weight',
'lang_embeddings.weight',
'embeddings.weight',
'layer_norm_emb.weight',
'layer_norm_emb.bias',
'attentions.0.q_lin.weight',
'attentions.0.q_lin.bias',
'attentions.0.k_lin.weight',
'attentions.0.k_lin.bias',
'attentions.0.v_lin.weight',
'attentions.0.v_lin.bias',
'attentions.0.out_lin.weight',
'attentions.0.out_lin.bias',
'attentions.1.q_lin.weight',
'attentions.1.q_lin.bias',
'attentions.1.k_lin.weight',
'attentions.1.k_lin.bias',
'attentions.1.v_lin.weight',
'attentions.1.v_lin.bias',
'attentions.1.out_lin.weight',
'attentions.1.out_lin.bias',
'attentions.2.q_lin.weight',
'attentions.2.q_lin.bias',
'attentions.2.k_lin.weight',
'attentions.2.k_lin.bias',
'attentions.2.v_lin.weight',
'attentions.2.v_lin.bias',
'attentions.2.out_lin.weight',
'attentions.2.out_lin.bias',
'attentions.3.q_lin.weight',
'attentions.3.q_lin.bias',
'attentions.3.k_lin.weight',
'attentions.3.k_lin.bias',
'attentions.3.v_lin.weight',
'attentions.3.v_lin.bias',
'attentions.3.out_lin.weight',
'attentions.3.out_lin.bias',
'attentions.4.q_lin.weight',
'attentions.4.q_lin.bias',
'attentions.4.k_lin.weight',
'attentions.4.k_lin.bias',
'attentions.4.v_lin.weight',
'attentions.4.v_lin.bias',
'attentions.4.out_lin.weight',
'attentions.4.out_lin.bias',
'attentions.5.q_lin.weight',
'attentions.5.q_lin.bias',
'attentions.5.k_lin.weight',
'attentions.5.k_lin.bias',
'attentions.5.v_lin.weight',
'attentions.5.v_lin.bias',
'attentions.5.out_lin.weight',
'attentions.5.out_lin.bias',
'layer_norm1.0.weight',
'layer_norm1.0.bias',
'layer_norm1.1.weight',
'layer_norm1.1.bias',
'layer_norm1.2.weight',
'layer_norm1.2.bias',
'layer_norm1.3.weight',
'layer_norm1.3.bias',
'layer_norm1.4.weight',
'layer_norm1.4.bias',
'layer_norm1.5.weight',
'layer_norm1.5.bias',
'ffns.0.lin1.weight',
'ffns.0.lin1.bias',
'ffns.0.lin2.weight',
'ffns.0.lin2.bias',
'ffns.1.lin1.weight',
'ffns.1.lin1.bias',
'ffns.1.lin2.weight',
'ffns.1.lin2.bias',
'ffns.2.lin1.weight',
'ffns.2.lin1.bias',
'ffns.2.lin2.weight',
'ffns.2.lin2.bias',
'ffns.3.lin1.weight',
'ffns.3.lin1.bias',
'ffns.3.lin2.weight',
'ffns.3.lin2.bias',
'ffns.4.lin1.weight',
'ffns.4.lin1.bias',
'ffns.4.lin2.weight',
'ffns.4.lin2.bias',
'ffns.5.lin1.weight',
'ffns.5.lin1.bias',
'ffns.5.lin2.weight',
'ffns.5.lin2.bias',
'layer_norm2.0.weight',
'layer_norm2.0.bias',
'layer_norm2.1.weight',
'layer_norm2.1.bias',
'layer_norm2.2.weight',
'layer_norm2.2.bias',
'layer_norm2.3.weight',
'layer_norm2.3.bias',
'layer_norm2.4.weight',
'layer_norm2.4.bias',
'layer_norm2.5.weight',
'layer_norm2.5.bias',
'pred_layer.proj.weight',
'pred_layer.proj.bias'
